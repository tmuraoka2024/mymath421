
---
output: 
  html_document:
  pdf_document: default
  word_document: default
title: "Assignment 10: Predictive Modeling - Part 1"
---

***How to do it?***: 

- Open the Rmarkdown file of this assignment ([link](assignment10.Rmd)) in Rstudio. 

- Right under each **question**, insert  a code chunk (you can use the hotkey `Ctrl + Alt + I` to add a code chunk) and code the solution for the question. 

- `Knit` the rmarkdown file (hotkey: `Ctrl + Alt + K`) to export an html.  

-  Publish the html file to your Githiub Page. 

***Submission***: Submit the link on Github of the assignment to Canvas

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE)
```


-------

1. Use the `Adult Census Income` dataset.  We will predict the income (whether or not it is more than 50k or not) of an adult. Import the dataset.  Partition the data into 80% training and 20% testing.  

```{r}
library(tidyverse)

df <- read_csv('adult_census_missing.csv')

df <- df %>% 
  rename(target=income) %>%
  mutate(target = as.factor(target),
         sex = as.factor(sex))

mean_age <- mean(df$age, na.rm=TRUE)
df$Age <- replace_na(df$age, mean_age)

df <- df %>% drop_na()
```

```{r}
library(caret)
set.seed(2020)
splitIndex <- createDataPartition(df$target, p = .80, 
                                  list = FALSE)
df_train1 <- df[ splitIndex,]
df_test1 <- df[-splitIndex,]
```

2. Practice Decision Tree.  Do the follows:

  - Use `rpart` package, create a decision tree with maximum depth of 3. 
  
  - Calculate the accuracy of the model on the testing data. Notice that the positive outcome here is not `1` but `>50K` or `<50K`. 
  
  - Plot the tree
  
  - Plot the variable importance by the tree
  
```{r}
library(rpart)
library(rattle)

tree_model1 <- rpart(target ~ ., data = df_train1,
                    control = rpart.control(maxdepth = 3))

fancyRpartPlot(tree_model1)
```

```{r}
tree_model1$variable.importance
```

```{r}
pred <- predict(tree_model1, df_test1, type = "class")

df_test1$target <- factor(df_test1$target, levels = levels(pred))

cm1 <- confusionMatrix(data = pred, reference = df_test1$target, positive = ">50K")
cm1$overall[1]
```
3. Create 3 more trees and compare the testing accuracy of these trees, which tree give the highest testing accuracy.

```{r}
set.seed(2017)
splitIndex <- createDataPartition(df$target, p = .80, 
                                  list = FALSE)
df_train2 <- df[ splitIndex,]
df_test2 <- df[-splitIndex,]

tree_model2 <- rpart(target ~ ., data = df_train2,
                    control = rpart.control(maxdepth = 3))

pred2 <- predict(tree_model2, df_test2, type = "class")

df_test2$target <- factor(df_test2$target, levels = levels(pred2))

cm2 <- confusionMatrix(data = pred2, reference = df_test2$target, positive = ">50K")
cm2$overall[1]
```

```{r}
set.seed(2007)
splitIndex <- createDataPartition(df$target, p = .80, 
                                  list = FALSE)
df_train3 <- df[ splitIndex,]
df_test3 <- df[-splitIndex,]

tree_model3 <- rpart(target ~ ., data = df_train3,
                    control = rpart.control(maxdepth = 3))

pred3 <- predict(tree_model3, df_test3, type = "class")

df_test3$target <- factor(df_test3$target, levels = levels(pred3))

cm3 <- confusionMatrix(data = pred3, reference = df_test3$target, positive = ">50K")
cm3$overall[1]
```

```{r}
set.seed(2029)
splitIndex <- createDataPartition(df$target, p = .80, 
                                  list = FALSE)
df_train4 <- df[ splitIndex,]
df_test4 <- df[-splitIndex,]

tree_model4 <- rpart(target ~ ., data = df_train4,
                    control = rpart.control(maxdepth = 3))

pred4 <- predict(tree_model4, df_test4, type = "class")

df_test4$target <- factor(df_test4$target, levels = levels(pred4))

cm4 <- confusionMatrix(data = pred4, reference = df_test4$target, positive = ">50K")
cm4$overall[1]
```

4. Practice Random Forest.  Do the follows: 

  - Use `randomForest` package, create a random forest of 1000 trees. 
  
  - Calculate the accuracy of the model on the testing data. 
  
  - Plot the variable importance by the forest
  
```{r}
library(randomForest)

forest_model1 <- randomForest(target ~ ., data=df_train1, ntree=500)
pred <- predict(forest_model1, df_test1, type = 'class')

cm <- confusionMatrix(data = pred, reference = df_test1$target, positive = '>50K')

cm$overall[1]
```


5. Create 3 more forests and compare the testing accuracy of these forests, which forest give the highest testing accuracy.

```{r}
forest_model2 <- randomForest(target ~ ., data=df_train2, ntree=500)
pred <- predict(forest_model2, df_test1, type = 'class')

cm <- confusionMatrix(data = pred, reference = df_test1$target, positive = '>50K')

cm$overall[1]
```

```{r}
forest_model3 <- randomForest(target ~ ., data=df_train3, ntree=500)
pred <- predict(forest_model3, df_test1, type = 'class')

cm <- confusionMatrix(data = pred, reference = df_test1$target, positive = '>50K')

cm$overall[1]
```

```{r}
forest_model4 <- randomForest(target ~ ., data=df_train4, ntree=500)
pred <- predict(forest_model4, df_test1, type = 'class')

cm <- confusionMatrix(data = pred, reference = df_test1$target, positive = '>50K')

cm$overall[1]
```

6. What is the best model (in term of testing accuracy) among all models (including trees and forests) you have trained?

The best model among all my models was my fourth forest model. All the forest models performed better than any of my individual tree models.


